train_total_batch_size: 8

# each: each gpu processes its own set subset of scenes; ignored when running on cpu
# common: all scenes are distributed across all gpus
train_data_mode: common  # each | common

train:
  total_batch_size: ${dataloader.train_total_batch_size}
  num_workers: 4
  shuffle: true
  pin_memory: false
val:
  total_batch_size: 4
  num_workers: 0
  pin_memory: false
test:
  total_batch_size: 4
  num_workers: 0
  pin_memory: false
